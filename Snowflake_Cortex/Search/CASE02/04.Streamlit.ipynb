{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP06.Streamlit Ïï± ÏÉùÏÑ±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python SDKÎ°ú ÏÑúÎπÑÏä§Î•º ÏøºÎ¶¨(snowflake Python Ìå®ÌÇ§ÏßÄ ÏÇ¨Ïö©)Ìï† Ïàò ÏûàÏäµÎãàÎã§. Ïù¥ ÏûêÏäµÏÑúÏóêÏÑúÎäî Streamlit in Snowflake Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏóêÏÑú Python SDKÎ•º ÏÇ¨Ïö©ÌïòÎäî Î∞©Î≤ïÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§.   \n",
    "\n",
    "Î®ºÏ†Ä ÏÑúÎπÑÏä§ ÏÉùÏÑ± Îã®Í≥ÑÏóêÏÑú Ï†ÑÏó≠ Snowsight UI Ïó≠Ìï†Ïù¥ ÏÑúÎπÑÏä§Î•º ÏÉùÏÑ±ÌïòÎäî Îç∞ ÏÇ¨Ïö©Îêú Ïó≠Ìï†Í≥º ÎèôÏùºÌïúÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§.   \n",
    "\n",
    "1. Snowsight Ïóê Î°úÍ∑∏Ïù∏Ìï©ÎãàÎã§.\n",
    "\n",
    "2. ÏôºÏ™Ω ÌÉêÏÉâ Î©îÎâ¥ÏóêÏÑú **Projects ¬ª Streamlit** Î•º ÏÑ†ÌÉùÌï©ÎãàÎã§.\n",
    "\n",
    "3. **+ Streamlit App** Î•º ÏÑ†ÌÉùÌï©ÎãàÎã§.\n",
    "\n",
    "4. Ï§ëÏöî: Ïï± ÏúÑÏπòÏóê ÎåÄÌïú cortex_search_tutorial_db Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏôÄ public Ïä§ÌÇ§ÎßàÎ•º ÏÑ†ÌÉùÌï©ÎãàÎã§.\n",
    "\n",
    "5. Streamlit in Snowflake Ìé∏ÏßëÍ∏∞Ïùò ÏôºÏ™Ω Ï∞ΩÏóêÏÑú **Packages** Î•º ÏÑ†ÌÉùÌïòÍ≥† snowflake (Î≤ÑÏ†Ñ >= 0.8.0)Î•º Ï∂îÍ∞ÄÌïòÏó¨ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏóê Ìå®ÌÇ§ÏßÄÎ•º ÏÑ§ÏπòÌï©ÎãàÎã§.\n",
    "\n",
    "6. ÏòàÏ†ú Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò ÏΩîÎìúÎ•º Îã§Ïùå Streamlit Ïï±ÏúºÎ°ú Î∞îÍøâÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from snowflake.core import Root # requires snowflake>=0.8.0\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "MODELS = [\n",
    "    \"mistral-large\",\n",
    "    \"snowflake-arctic\",\n",
    "    \"llama3-70b\",\n",
    "    \"llama3-8b\",\n",
    "]\n",
    "\n",
    "def init_messages():\n",
    "    \"\"\"\n",
    "    Initialize the session state for chat messages. If the session state indicates that the\n",
    "    conversation should be cleared or if the \"messages\" key is not in the session state,\n",
    "    initialize it as an empty list.\n",
    "    \"\"\"\n",
    "    if st.session_state.clear_conversation or \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "def init_service_metadata():\n",
    "    \"\"\"\n",
    "    Initialize the session state for cortex search service metadata. Query the available\n",
    "    cortex search services from the Snowflake session and store their names and search\n",
    "    columns in the session state.\n",
    "    \"\"\"\n",
    "    if \"service_metadata\" not in st.session_state:\n",
    "        services = session.sql(\"SHOW CORTEX SEARCH SERVICES;\").collect()\n",
    "        service_metadata = []\n",
    "        if services:\n",
    "            for s in services:\n",
    "                svc_name = s[\"name\"]\n",
    "                svc_search_col = session.sql(\n",
    "                    f\"DESC CORTEX SEARCH SERVICE {svc_name};\"\n",
    "                ).collect()[0][\"search_column\"]\n",
    "                service_metadata.append(\n",
    "                    {\"name\": svc_name, \"search_column\": svc_search_col}\n",
    "                )\n",
    "\n",
    "        st.session_state.service_metadata = service_metadata\n",
    "\n",
    "def init_config_options():\n",
    "    \"\"\"\n",
    "    Initialize the configuration options in the Streamlit sidebar. Allow the user to select\n",
    "    a cortex search service, clear the conversation, toggle debug mode, and toggle the use of\n",
    "    chat history. Also provide advanced options to select a model, the number of context chunks,\n",
    "    and the number of chat messages to use in the chat history.\n",
    "    \"\"\"\n",
    "    st.sidebar.selectbox(\n",
    "        \"Select cortex search service:\",\n",
    "        [s[\"name\"] for s in st.session_state.service_metadata],\n",
    "        key=\"selected_cortex_search_service\",\n",
    "    )\n",
    "\n",
    "    st.sidebar.button(\"Clear conversation\", key=\"clear_conversation\")\n",
    "    st.sidebar.toggle(\"Debug\", key=\"debug\", value=False)\n",
    "    st.sidebar.toggle(\"Use chat history\", key=\"use_chat_history\", value=True)\n",
    "\n",
    "    with st.sidebar.expander(\"Advanced options\"):\n",
    "        st.selectbox(\"Select model:\", MODELS, key=\"model_name\")\n",
    "        st.number_input(\n",
    "            \"Select number of context chunks\",\n",
    "            value=5,\n",
    "            key=\"num_retrieved_chunks\",\n",
    "            min_value=1,\n",
    "            max_value=10,\n",
    "        )\n",
    "        st.number_input(\n",
    "            \"Select number of messages to use in chat history\",\n",
    "            value=5,\n",
    "            key=\"num_chat_messages\",\n",
    "            min_value=1,\n",
    "            max_value=10,\n",
    "        )\n",
    "\n",
    "    st.sidebar.expander(\"Session State\").write(st.session_state)\n",
    "\n",
    "def query_cortex_search_service(query):\n",
    "    \"\"\"\n",
    "    Query the selected cortex search service with the given query and retrieve context documents.\n",
    "    Display the retrieved context documents in the sidebar if debug mode is enabled. Return the\n",
    "    context documents as a string.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to search the cortex search service with.\n",
    "\n",
    "    Returns:\n",
    "        str: The concatenated string of context documents.\n",
    "    \"\"\"\n",
    "    db, schema = session.get_current_database(), session.get_current_schema()\n",
    "\n",
    "    cortex_search_service = (\n",
    "        root.databases[db]\n",
    "        .schemas[schema]\n",
    "        .cortex_search_services[st.session_state.selected_cortex_search_service]\n",
    "    )\n",
    "\n",
    "    context_documents = cortex_search_service.search(\n",
    "        query, columns=[], limit=st.session_state.num_retrieved_chunks\n",
    "    )\n",
    "    results = context_documents.results\n",
    "\n",
    "    service_metadata = st.session_state.service_metadata\n",
    "    search_col = [s[\"search_column\"] for s in service_metadata\n",
    "                    if s[\"name\"] == st.session_state.selected_cortex_search_service][0]\n",
    "\n",
    "    context_str = \"\"\n",
    "    for i, r in enumerate(results):\n",
    "        context_str += f\"Context document {i+1}: {r[search_col]} \\n\" + \"\\n\"\n",
    "\n",
    "    if st.session_state.debug:\n",
    "        st.sidebar.text_area(\"Context documents\", context_str, height=500)\n",
    "\n",
    "    return context_str\n",
    "\n",
    "def get_chat_history():\n",
    "    \"\"\"\n",
    "    Retrieve the chat history from the session state limited to the number of messages specified\n",
    "    by the user in the sidebar options.\n",
    "\n",
    "    Returns:\n",
    "        list: The list of chat messages from the session state.\n",
    "    \"\"\"\n",
    "    start_index = max(\n",
    "        0, len(st.session_state.messages) - st.session_state.num_chat_messages\n",
    "    )\n",
    "    return st.session_state.messages[start_index : len(st.session_state.messages) - 1]\n",
    "\n",
    "def complete(model, prompt):\n",
    "    \"\"\"\n",
    "    Generate a completion for the given prompt using the specified model.\n",
    "\n",
    "    Args:\n",
    "        model (str): The name of the model to use for completion.\n",
    "        prompt (str): The prompt to generate a completion for.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated completion.\n",
    "    \"\"\"\n",
    "    return session.sql(\"SELECT snowflake.cortex.complete(?,?)\", (model, prompt)).collect()[0][0]\n",
    "\n",
    "def make_chat_history_summary(chat_history, question):\n",
    "    \"\"\"\n",
    "    Generate a summary of the chat history combined with the current question to extend the query\n",
    "    context. Use the language model to generate this summary.\n",
    "\n",
    "    Args:\n",
    "        chat_history (str): The chat history to include in the summary.\n",
    "        question (str): The current user question to extend with the chat history.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated summary of the chat history and question.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "        [INST]\n",
    "        Based on the chat history below and the question, generate a query that extend the question\n",
    "        with the chat history provided. The query should be in natural language.\n",
    "        Answer with only the query. Do not add any explanation.\n",
    "\n",
    "        <chat_history>\n",
    "        {chat_history}\n",
    "        </chat_history>\n",
    "        <question>\n",
    "        {question}\n",
    "        </question>\n",
    "        [/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    summary = complete(st.session_state.model_name, prompt)\n",
    "\n",
    "    if st.session_state.debug:\n",
    "        st.sidebar.text_area(\n",
    "            \"Chat history summary\", summary.replace(\"$\", \"\\$\"), height=150\n",
    "        )\n",
    "\n",
    "    return summary\n",
    "\n",
    "def create_prompt(user_question):\n",
    "    \"\"\"\n",
    "    Create a prompt for the language model by combining the user question with context retrieved\n",
    "    from the cortex search service and chat history (if enabled). Format the prompt according to\n",
    "    the expected input format of the model.\n",
    "\n",
    "    Args:\n",
    "        user_question (str): The user's question to generate a prompt for.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prompt for the language model.\n",
    "    \"\"\"\n",
    "    if st.session_state.use_chat_history:\n",
    "        chat_history = get_chat_history()\n",
    "        if chat_history != []:\n",
    "            question_summary = make_chat_history_summary(chat_history, user_question)\n",
    "            prompt_context = query_cortex_search_service(question_summary)\n",
    "        else:\n",
    "            prompt_context = query_cortex_search_service(user_question)\n",
    "    else:\n",
    "        prompt_context = query_cortex_search_service(user_question)\n",
    "        chat_history = \"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "            [INST]\n",
    "            You are a helpful AI chat assistant with RAG capabilities. When a user asks you a question,\n",
    "            you will also be given context provided between <context> and </context> tags. Use that context\n",
    "            with the user's chat history provided in the between <chat_history> and </chat_history> tags\n",
    "            to provide a summary that addresses the user's question. Ensure the answer is coherent, concise,\n",
    "            and directly relevant to the user's question.\n",
    "\n",
    "            If the user asks a generic question which cannot be answered with the given context or chat_history,\n",
    "            just say \"I don't know the answer to that question.\n",
    "\n",
    "            Don't saying things like \"according to the provided context\".\n",
    "\n",
    "            <chat_history>\n",
    "            {chat_history}\n",
    "            </chat_history>\n",
    "            <context>\n",
    "            {prompt_context}\n",
    "            </context>\n",
    "            <question>\n",
    "            {user_question}\n",
    "            </question>\n",
    "            [/INST]\n",
    "            Answer:\n",
    "        \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def main():\n",
    "    st.title(f\":speech_balloon: Chatbot with Snowflake Cortex\")\n",
    "\n",
    "    init_service_metadata()\n",
    "    init_config_options()\n",
    "    init_messages()\n",
    "\n",
    "    icons = {\"assistant\": \"‚ùÑÔ∏è\", \"user\": \"üë§\"}\n",
    "\n",
    "    # Display chat messages from history on app rerun\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"], avatar=icons[message[\"role\"]]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    disable_chat = (\n",
    "        \"service_metadata\" not in st.session_state\n",
    "        or len(st.session_state.service_metadata) == 0\n",
    "    )\n",
    "    if question := st.chat_input(\"Ask a question...\", disabled=disable_chat):\n",
    "        # Add user message to chat history\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
    "        # Display user message in chat message container\n",
    "        with st.chat_message(\"user\", avatar=icons[\"user\"]):\n",
    "            st.markdown(question.replace(\"$\", \"\\$\"))\n",
    "\n",
    "        # Display assistant response in chat message container\n",
    "        with st.chat_message(\"assistant\", avatar=icons[\"assistant\"]):\n",
    "            message_placeholder = st.empty()\n",
    "            question = question.replace(\"'\", \"\")\n",
    "            with st.spinner(\"Thinking...\"):\n",
    "                generated_response = complete(\n",
    "                    st.session_state.model_name, create_prompt(question)\n",
    "                )\n",
    "                message_placeholder.markdown(generated_response)\n",
    "\n",
    "        st.session_state.messages.append(\n",
    "            {\"role\": \"assistant\", \"content\": generated_response}\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    session = get_active_session()\n",
    "    root = Root(session)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ïï± ÏÇ¨Ïö©Ìï¥ Î≥¥Í∏∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÌÖçÏä§Ìä∏ ÏÉÅÏûêÏóê ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•ÌïòÏó¨ ÏÉà Ïï±ÏùÑ ÏÇ¨Ïö©Ìï¥ Î¥ÖÎãàÎã§. ÏãúÎèÑÌï¥ Î≥º Ïàò ÏûàÎäî Î™á Í∞ÄÏßÄ ÏÉòÌîå ÏøºÎ¶¨Îäî Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.\n",
    "\n",
    "_I like Harry Potter. Can you recommend more books I will like?_\n",
    "\n",
    "_Can you recommend me books on Greek Mythology?_"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
